WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.580
So before you start with a project tasks let me

00:00:02.580 --> 00:00:04.620
give you a tour of the student version of a code.

00:00:04.620 --> 00:00:06.689
You will already be familiar with the basic structure as

00:00:06.690 --> 00:00:08.940
it has been built on top of the midterm projects.

00:00:08.939 --> 00:00:10.709
I suggest that you grab a copy of a program

00:00:10.710 --> 00:00:12.870
schematic and look at it while we go through the code.

00:00:12.869 --> 00:00:17.750
So let's start. So the basis of this code is basically the midterm projects.

00:00:17.750 --> 00:00:20.609
So you should be familiar with many of the structures and

00:00:20.609 --> 00:00:24.449
includes and also initializations we have been using previously as well.

00:00:24.449 --> 00:00:26.189
But let's start from the top.

00:00:26.190 --> 00:00:29.280
So we have a whole bunch of includes, including STL includes,

00:00:29.280 --> 00:00:31.019
as well as OpenCV includes here,

00:00:31.019 --> 00:00:33.939
and also we have our proprietary includes,

00:00:33.939 --> 00:00:36.960
which are basically the different parts of

00:00:36.960 --> 00:00:40.439
the projects which are concerned with 2D matching,

00:00:40.439 --> 00:00:42.390
object detection using yolo,

00:00:42.390 --> 00:00:46.579
processing lidar data and also performing fusion between

00:00:46.579 --> 00:00:51.164
lidar and camera data which is all in camFusion.hpp.

00:00:51.164 --> 00:00:53.629
So let's start with main program first.

00:00:53.630 --> 00:00:56.195
So the first thing which we do here is we initialize

00:00:56.195 --> 00:00:59.750
the variables and data structures which we need for our project.

00:00:59.750 --> 00:01:01.369
The first is the data path,

00:01:01.369 --> 00:01:05.689
it's currently relative to the location of the executable.

00:01:05.689 --> 00:01:08.929
Then, we specify the image base path which is

00:01:08.930 --> 00:01:13.540
also the image prefix which is currently such as the KITTI data search.

00:01:13.540 --> 00:01:16.790
That's the highway scenario with the preceding vehicle right in front of us.

00:01:16.790 --> 00:01:21.440
Image dash O2 means it's the color version of the image,

00:01:21.439 --> 00:01:22.640
not the grayscale version.

00:01:22.640 --> 00:01:25.189
I think in the midterm projects we have been

00:01:25.189 --> 00:01:28.420
using the grayscale version but now we are using the color version.

00:01:28.420 --> 00:01:33.844
Then, we are specifying the start index and the end index which is currently set to 18.

00:01:33.844 --> 00:01:36.439
Step width is a new variable which you can

00:01:36.439 --> 00:01:38.959
use to play around a little to decrease the frame rate.

00:01:38.959 --> 00:01:41.944
So our current frame rate is set to 10 images per second.

00:01:41.944 --> 00:01:45.139
But we might skip every second image just to have

00:01:45.140 --> 00:01:48.349
larger disparities between key points or maybe

00:01:48.349 --> 00:01:54.724
to see how matching can be performed if the time between two instances increases.

00:01:54.724 --> 00:01:56.000
So you can play around a little bit,

00:01:56.000 --> 00:01:58.849
but currently in this project step width is set to one.

00:01:58.849 --> 00:02:01.434
So we are using every image.

00:02:01.435 --> 00:02:05.090
Then we specify the base parameters of yolo.

00:02:05.090 --> 00:02:06.530
Base path, classes file,

00:02:06.530 --> 00:02:10.814
model configuration file also the model weights you have been using those previously.

00:02:10.814 --> 00:02:14.379
Then, we specify the lidar up point data,

00:02:14.379 --> 00:02:16.430
the prefix as well as the file type.

00:02:16.430 --> 00:02:18.890
Then, we know this whole bunch of calibration information which

00:02:18.889 --> 00:02:21.599
you already know from the previous exercises.

00:02:21.599 --> 00:02:25.789
Basically, it's the intrinsic camera calibration matrix,

00:02:25.789 --> 00:02:27.590
the rotation matrix for relating left and

00:02:27.590 --> 00:02:30.629
right camera of the stereo setup in the KITTI vehicle.

00:02:30.629 --> 00:02:34.969
Then, we have the extrinsic information which is rotation and translation

00:02:34.969 --> 00:02:40.085
and these are all the numbers which I took from the KITTI camera calibration file.

00:02:40.085 --> 00:02:43.099
Which has been shipped with the image sequence we are using.

00:02:43.099 --> 00:02:46.189
If you want to use another sequence please remember to replace

00:02:46.189 --> 00:02:49.400
those with the respective calibration files which are

00:02:49.400 --> 00:02:53.885
the same file folder when you download and extract one of the KITTI sensor setups

00:02:53.884 --> 00:02:58.750
or sensor sequences you will find the calibration files there.

00:02:58.750 --> 00:03:00.629
Okay. So what else?

00:03:00.629 --> 00:03:04.009
We have some miscellaneous variables here which is the sense of

00:03:04.009 --> 00:03:09.094
frame rate 10 hertz scaled by the image step width.

00:03:09.094 --> 00:03:11.044
So if we go down to let's say

00:03:11.044 --> 00:03:16.359
only every second image this gets scaled down to five hertz accordingly.

00:03:16.360 --> 00:03:21.230
Okay. Data buffer size currently two and this is the data buffer which is going

00:03:21.229 --> 00:03:26.554
to hold all the data frames which we will assemble in the following.

00:03:26.555 --> 00:03:30.155
This visualization variable here it's always set to false and

00:03:30.155 --> 00:03:35.689
my suggestion is to set it to true only at selected positions in the code.

00:03:35.689 --> 00:03:39.109
If you set it to true here you're going to get a display of

00:03:39.110 --> 00:03:41.090
a whole bunch of different windows and it's a

00:03:41.090 --> 00:03:43.474
little bit hard to keep track of what is what.

00:03:43.474 --> 00:03:49.479
So I would suggest to look at all the codes steps one by one and only set

00:03:49.479 --> 00:03:52.819
visualization true right in front of the respective step which you want

00:03:52.819 --> 00:03:56.289
to visualize and to false directly afterwards.

00:03:56.289 --> 00:03:59.644
Okay. Then, we have the main image loop of all the images.

00:03:59.645 --> 00:04:02.570
We do load the images into a buffer.

00:04:02.569 --> 00:04:06.439
Note that here's a small difference to the midterm project.

00:04:06.439 --> 00:04:08.930
We do read the image from file here but it's

00:04:08.930 --> 00:04:12.740
the color image and we do not convert to grayscale right away but instead,

00:04:12.740 --> 00:04:17.060
we push back the frame including the color image.

00:04:17.060 --> 00:04:18.280
That's the difference.

00:04:18.279 --> 00:04:22.669
Okay. Then, comes the part where we detect and classify the objects.

00:04:22.670 --> 00:04:24.920
We have some parameters here: confidence,

00:04:24.920 --> 00:04:27.560
threshold, and maximum suppression threshold.

00:04:27.560 --> 00:04:31.685
Then, we perform the yolo based object detection here.

00:04:31.685 --> 00:04:35.990
Which is done in this step and note that we

00:04:35.990 --> 00:04:41.000
are inputting a link to the data buffer camera image,

00:04:41.000 --> 00:04:42.800
data buffer bounding boxes,

00:04:42.800 --> 00:04:44.870
and the data buffer bounding boxes,

00:04:44.870 --> 00:04:49.040
they are augmented with the rectangle around detected objects.

00:04:49.040 --> 00:04:53.750
So that's basically the output of the detect objects method.

00:04:53.750 --> 00:04:57.620
We are adding in rectangles into the bounding boxes here.

00:04:57.620 --> 00:05:01.665
In the next step, we are going to crop all lidar point cloud.

00:05:01.665 --> 00:05:04.785
Well, first of all, we're going to load the lidar point from file.

00:05:04.785 --> 00:05:07.640
Then, we are going to remove some points

00:05:07.639 --> 00:05:11.104
based on some special properties here crop lidar points.

00:05:11.105 --> 00:05:13.560
Currently, the code is said to have a minZ,

00:05:13.560 --> 00:05:17.149
a minHeight value above road surface of minus 1.5 meters,

00:05:17.149 --> 00:05:19.729
that's slightly above road surface.

00:05:19.730 --> 00:05:24.290
If you had a sequence with an inclined when you are driving up

00:05:24.290 --> 00:05:30.004
a steep slope or something like this or a very deep pothole, so to speak,

00:05:30.004 --> 00:05:31.594
or a bump on the road,

00:05:31.595 --> 00:05:34.100
then this would lead to errors in

00:05:34.100 --> 00:05:38.750
the TTC computation later on because by doing this very simple cropping here

00:05:38.750 --> 00:05:42.199
basically what we do is we assume a level road surface

00:05:42.199 --> 00:05:46.784
and if the reality deviates strongly from this assumption,

00:05:46.785 --> 00:05:50.750
we would have to take different measures to avoid

00:05:50.750 --> 00:05:53.449
using points from the road surface as

00:05:53.449 --> 00:05:56.659
obstacle information but this leads to far for this course.

00:05:56.660 --> 00:06:01.805
So I'm just mentioning this because I want you to know that the working assumption we are

00:06:01.805 --> 00:06:04.340
having here is that we are assuming

00:06:04.339 --> 00:06:08.484
a level road surface for the sequences we process here.

00:06:08.485 --> 00:06:13.470
Okay. The maxZ value is minus 0.9 meters.

00:06:13.470 --> 00:06:17.180
Into driving direction, we start looking onwards from

00:06:17.180 --> 00:06:21.319
two meters to 20 meters and in y we are looking to

00:06:21.319 --> 00:06:24.740
the left for two meters and to the right also for two meters and

00:06:24.740 --> 00:06:32.370
the minimum reflectivity has been set to 0.1 or in range between zero and one.

00:06:32.370 --> 00:06:37.399
One is the highest reflectivity and 0.0 is the lowest reflectivity.

00:06:37.399 --> 00:06:39.469
But it's still a point which comes back.

00:06:39.470 --> 00:06:42.860
So, 0.0 it does not mean there is nothing coming back.

00:06:42.860 --> 00:06:46.420
It's still a valid point but only the reflectivity is really null.

00:06:46.420 --> 00:06:49.050
We are focusing on the ego lane here,

00:06:49.050 --> 00:06:53.314
so the goal is to compute the TTC and not to display all the objects.

00:06:53.314 --> 00:06:55.040
We are focusing on the ego lane.

00:06:55.040 --> 00:06:57.950
If you want to include a left and right lane and also look a bit

00:06:57.949 --> 00:07:01.490
further maybe you can of course play around with these values here

00:07:01.490 --> 00:07:08.879
but for the point cloud based TTC estimation later on we have to focus on the ego lane.

00:07:08.879 --> 00:07:11.839
Okay. Then, we push the lidar points into

00:07:11.839 --> 00:07:16.069
the data buffer and then we move on to clustering the actual lidar point clouds.

00:07:16.069 --> 00:07:17.899
We have discuss this function before,

00:07:17.899 --> 00:07:20.164
cluster lidar with ROI.

00:07:20.165 --> 00:07:24.650
The shrink factor relates to the bounding box or the region of interest,

00:07:24.649 --> 00:07:27.349
size of the returned yolo objects.

00:07:27.350 --> 00:07:30.365
We're making it a bit smaller by 10 percent.

00:07:30.365 --> 00:07:35.660
Just to avoid that points from unrelated objects creep in over the corners,

00:07:35.660 --> 00:07:38.510
over the boundaries of the rectangles which are

00:07:38.509 --> 00:07:43.949
almost all the times when you look at the objects they are a little bit too large.

00:07:43.949 --> 00:07:47.709
So we shrink them here to avoid this from happening a little bit.

00:07:47.709 --> 00:07:50.764
Then, we have the first visualization here.

00:07:50.764 --> 00:07:52.279
Visualize 3D objects.

00:07:52.279 --> 00:07:56.389
So, actually if you set bVis to true right

00:07:56.389 --> 00:08:01.669
before then we will focus only on showing the 3D objects,

00:08:01.670 --> 00:08:05.120
which is a display which you already know from the course lectures.

00:08:05.120 --> 00:08:08.930
You saw a screenshot of this white's top view perspective with

00:08:08.930 --> 00:08:12.650
all the 3D objects visualized as rectangles with

00:08:12.649 --> 00:08:17.419
some information and you can run this code and set bVis to true and

00:08:17.420 --> 00:08:22.465
then you will get the same visual as you saw in the lecture as a still image.

00:08:22.464 --> 00:08:23.959
Okay. That's basically about it.

00:08:23.959 --> 00:08:27.424
That's the state of the affairs so to speak,

00:08:27.425 --> 00:08:30.800
before the mid-term project actually starts.

00:08:30.800 --> 00:08:35.779
The goal of the midterm project is to have you track 3D objects over time and what we

00:08:35.779 --> 00:08:41.199
get until this point we create 3D objects for each timestep individually,

00:08:41.200 --> 00:08:44.375
and if you just want to run the code and see what's happening,

00:08:44.375 --> 00:08:48.710
you can execute it and at this line here we basically spare

00:08:48.710 --> 00:08:52.850
out the entire stuff which happens right afterwards from happening.

00:08:52.850 --> 00:08:56.134
So we can only focus on creating 3D objects using

00:08:56.134 --> 00:09:00.200
object protection based on yolo and clustering lidar points.

00:09:00.200 --> 00:09:05.000
If you want to move on you have to comment out this continue statement here and then we

00:09:05.000 --> 00:09:10.625
progress to the actual part where the final project starts and revolves around.

00:09:10.625 --> 00:09:14.210
But before we get to your task and the final project let's

00:09:14.210 --> 00:09:17.795
look at what's already being done here.

00:09:17.794 --> 00:09:19.639
What you already have done.

00:09:19.639 --> 00:09:22.384
So the first step is to detect image key points.

00:09:22.384 --> 00:09:26.075
This is now the step where we converge the image to grayscale.

00:09:26.075 --> 00:09:30.230
We're not doing this for the yolo-based object detection, actually,

00:09:30.230 --> 00:09:33.935
the results for the deep learning framework

00:09:33.934 --> 00:09:36.589
are much worse if we only input grayscale image.

00:09:36.590 --> 00:09:39.139
It has been trained with color and if we

00:09:39.139 --> 00:09:42.080
take away the color the results are really much worse.

00:09:42.080 --> 00:09:45.985
You can try this out and see why it makes sense to use the color image instead.

00:09:45.985 --> 00:09:48.195
But here for key point detection,

00:09:48.195 --> 00:09:50.610
we actually do need grayscale images,

00:09:50.610 --> 00:09:52.909
which is why we convert them to grayscale here.

00:09:52.909 --> 00:09:56.000
Okay. Then, comes the part where you can select a detector.

00:09:56.000 --> 00:09:59.960
As you can see here this is the student version of the midterm project.

00:09:59.960 --> 00:10:03.080
You should now have a coding framework that has a much

00:10:03.080 --> 00:10:06.590
more included here including brisk and areas and sift

00:10:06.590 --> 00:10:09.500
and all the other detectors which you were tasked with implementing

00:10:09.500 --> 00:10:13.490
and once you download this final project student version here,

00:10:13.490 --> 00:10:15.560
you can simply copy and paste your version of

00:10:15.559 --> 00:10:18.379
the midterm into this area here and you would have

00:10:18.379 --> 00:10:24.169
the same key point detection and tracking method as you now have in the midterm project.

00:10:24.169 --> 00:10:25.954
Okay. That's key point detection.

00:10:25.955 --> 00:10:28.520
Then, of course, we can also limit the number of

00:10:28.519 --> 00:10:31.439
key points here but that doesn't make much sense at this point,

00:10:31.440 --> 00:10:33.320
so I would leave this set to false but

00:10:33.320 --> 00:10:36.560
nonetheless if you want to visualize key point matches you can set this

00:10:36.559 --> 00:10:39.709
to true and this is going to limit the number of key points

00:10:39.710 --> 00:10:43.540
to 50 which gives you a better overview of what's happening.

00:10:43.539 --> 00:10:46.889
So next step is extract key point descriptors,

00:10:46.889 --> 00:10:51.164
and this is also already familiar we can select a whole bunch of descriptors here

00:10:51.164 --> 00:10:56.730
and desk key points is inside matching 2D,

00:10:56.730 --> 00:11:01.149
the student version and you could of course replace this function with

00:11:01.149 --> 00:11:03.059
your own code from the midterm Project which

00:11:03.059 --> 00:11:05.419
gives you access to all those different descriptors here.

00:11:05.419 --> 00:11:09.485
Currently, we only have brisk available in the Final Project source code.

00:11:09.485 --> 00:11:13.240
So once this is done and we have at least processed two images,

00:11:13.240 --> 00:11:15.720
this is the place where we enter this area here,

00:11:15.720 --> 00:11:18.625
data buffer size greater than one larger than one.

00:11:18.625 --> 00:11:20.839
The first thing we do is we match the descriptors

00:11:20.839 --> 00:11:23.550
and here goes the same what I just mentioned,

00:11:23.549 --> 00:11:25.839
you can take your version from the midterm Project and

00:11:25.840 --> 00:11:28.379
replace match descriptors with your own code which has

00:11:28.379 --> 00:11:31.029
all the stuff implemented which you can see here

00:11:31.029 --> 00:11:35.429
the flood based matching and also the k-nearest neighbor based matching,

00:11:35.429 --> 00:11:38.474
which you have been doing for the matching project.

00:11:38.475 --> 00:11:40.519
So here comes the first assignment,

00:11:40.519 --> 00:11:45.889
up until this point what we do have is a set of 3D objects in space,

00:11:45.889 --> 00:11:49.019
one for each time step and we have a whole set

00:11:49.019 --> 00:11:52.329
of key point matches between images all over

00:11:52.330 --> 00:11:55.290
the image without having looked at a single region of

00:11:55.289 --> 00:11:59.309
interests or bounding box or object yet that has not been done.

00:11:59.309 --> 00:12:02.734
What I want you to do as the first assignment and we're going to look into this

00:12:02.735 --> 00:12:06.245
in greater detail once we get to this respective task in a few minutes,

00:12:06.245 --> 00:12:08.460
I want you to write a function which is

00:12:08.460 --> 00:12:11.215
called match bounding boxes which takes as an input,

00:12:11.215 --> 00:12:12.720
the previous data buffer,

00:12:12.720 --> 00:12:17.980
the current data buffer and that gives you as output a map which contains

00:12:17.980 --> 00:12:21.879
the index numbers of the bounding boxes

00:12:21.879 --> 00:12:26.120
which have been found to corresponds together between two time steps,

00:12:26.120 --> 00:12:27.740
that's the first task.

00:12:27.740 --> 00:12:33.799
Then we write those bounding box best matches into the current data buffer.

00:12:33.799 --> 00:12:35.564
That's what's happening here.

00:12:35.565 --> 00:12:38.130
Then we move on to the next part,

00:12:38.129 --> 00:12:43.814
which is basically the computation of the TTC on the object right in front.

00:12:43.815 --> 00:12:48.150
What we do here is we loop over all bounding box match pairs which we have font,

00:12:48.149 --> 00:12:51.629
so bbMatches begin to bbMatches end.

00:12:51.629 --> 00:12:54.179
That's the result of your first task,

00:12:54.179 --> 00:12:59.109
which is looped over here and this block of code here what it does is

00:12:59.110 --> 00:13:05.220
basically it gives you a pointer to the bounding box in the current frame,

00:13:05.220 --> 00:13:07.654
the first match pair in the current frame.

00:13:07.654 --> 00:13:13.414
This code here by comparing bounding box IDs to other bounding boxes in our data buffer,

00:13:13.414 --> 00:13:15.419
we get the previous bounding box.

00:13:15.419 --> 00:13:20.324
So basically, after arriving here in line 254,

00:13:20.325 --> 00:13:22.064
what you have is two pointers,

00:13:22.063 --> 00:13:25.009
one pointer to a bounding box in the previous frame

00:13:25.009 --> 00:13:28.460
and another pointer to a bounding box in the current frame

00:13:28.460 --> 00:13:31.430
and both have been matched using

00:13:31.429 --> 00:13:35.384
the method you would have implemented here in match bounding boxes.

00:13:35.384 --> 00:13:37.294
Once you have this match pair,

00:13:37.294 --> 00:13:40.504
what we can do or what we must do is we must check whether

00:13:40.504 --> 00:13:44.299
both bounding boxes have lidar points associated to them.

00:13:44.299 --> 00:13:50.279
We couldn't do lidar Base TTC estimation if a bounding box was devoid of lidar points.

00:13:50.279 --> 00:13:54.759
So we are checking whether the current bounding box has lidar points associated to it,

00:13:54.759 --> 00:13:56.669
as well as the previous bounding box.

00:13:56.669 --> 00:13:58.714
Then comes your second student assignment.

00:13:58.715 --> 00:14:00.129
So what I want you to do now,

00:14:00.129 --> 00:14:05.345
is I want you to compute the time to collision based on lidar measurements alone.

00:14:05.345 --> 00:14:07.800
As an input, you would take the lidar points and

00:14:07.799 --> 00:14:10.995
the previous bounding box lidar points and the current bounding box,

00:14:10.995 --> 00:14:12.570
the sense of frame rate,

00:14:12.570 --> 00:14:19.825
and an output as a result you would fill in this variable here which is called TTC Lidar.

00:14:19.825 --> 00:14:24.120
Now in the next student assignment task three and also task four,

00:14:24.120 --> 00:14:25.485
they both go together,

00:14:25.485 --> 00:14:30.180
you would need to associate key point matches with bounding boxes.

00:14:30.179 --> 00:14:34.199
So currently we have bounding boxes which only have been associated a set of

00:14:34.200 --> 00:14:37.085
lidar points and what we need in order to perform

00:14:37.085 --> 00:14:40.440
TTC based estimation using the camera for each bounding box,

00:14:40.440 --> 00:14:45.545
we need a set of matched key points which are enclosed by a specific bounding box.

00:14:45.544 --> 00:14:49.019
We will look into this a bit more closely once we got to the respective task.

00:14:49.019 --> 00:14:52.289
But the basic idea here is to look at

00:14:52.289 --> 00:14:56.204
the region of interest for a certain bounding box in the camera image,

00:14:56.205 --> 00:14:58.800
look at which key point correspondences

00:14:58.799 --> 00:15:02.979
key point matches are enclosed within this region of interest,

00:15:02.980 --> 00:15:07.230
and then take those and associate them to the bounding box,

00:15:07.230 --> 00:15:11.460
with the idea behind that we will use those key point matches to find

00:15:11.460 --> 00:15:13.139
the corresponding bounding box in

00:15:13.139 --> 00:15:16.745
the next image to associate the two bounding boxes over time.

00:15:16.745 --> 00:15:20.625
That's the end of the fourth student assignment.

00:15:20.625 --> 00:15:23.235
Basically, that's also the end of the final project.

00:15:23.235 --> 00:15:26.909
What's happening here is here you will be visualizing the results.

00:15:26.909 --> 00:15:30.969
We will be looking at an overlay of lidar points over the image,

00:15:30.970 --> 00:15:34.600
we will have a view on the detected rectangle which

00:15:34.600 --> 00:15:38.250
is the display of the respective region of interests,

00:15:38.250 --> 00:15:40.924
that means the detected vehicle or a truck or whatever,

00:15:40.924 --> 00:15:42.620
it might have been present in the image.

00:15:42.620 --> 00:15:45.029
Then we will augment the image with information

00:15:45.029 --> 00:15:47.699
about the time to collision computed from lidar,

00:15:47.700 --> 00:15:49.545
time to collision could be from camera,

00:15:49.544 --> 00:15:52.674
and that's the display code for the actual.

00:15:52.674 --> 00:15:56.654
Now, as you have seen the code is basically an extended version of the mid-term projects.

00:15:56.654 --> 00:15:59.289
Also the major concepts of the last lesson which you should know

00:15:59.289 --> 00:16:02.099
from some of the exercises have already been implemented.

00:16:02.100 --> 00:16:03.960
So let's now move forward to the first task,

00:16:03.960 --> 00:16:09.430
which is about matching 3D objects over time using key point correspondences.

