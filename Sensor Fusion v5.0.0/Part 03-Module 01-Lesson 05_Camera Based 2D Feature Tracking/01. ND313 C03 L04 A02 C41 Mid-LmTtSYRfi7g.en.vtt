WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.459
So before you start with your first task,

00:00:02.459 --> 00:00:05.099
I would like to give you a tour of a code you will be working on.

00:00:05.099 --> 00:00:08.490
Now the input of the data processing chain is a sequence of 10 images.

00:00:08.490 --> 00:00:11.655
The output is a set of nine matched key point pairs.

00:00:11.654 --> 00:00:15.464
The code is currently using the Shi-Tomasi Detector and the BRISK descriptor.

00:00:15.464 --> 00:00:18.105
Once we have the descriptors for two successive images,

00:00:18.105 --> 00:00:20.984
brute-force matching is used to associate key points.

00:00:20.984 --> 00:00:25.324
Now let's take a look at the code so you can start right away working on your first task.

00:00:25.324 --> 00:00:29.554
Okay. So what you can see here is the project space which we will be using.

00:00:29.554 --> 00:00:32.750
The way we will do this is we will walk you through from top to bottom in

00:00:32.750 --> 00:00:36.695
the main file, which is MidTermProject_Camera_Student.cpp.

00:00:36.695 --> 00:00:39.750
Now at the start of this file are

00:00:39.750 --> 00:00:43.070
the includes which our basic STL includes such as iostream,

00:00:43.070 --> 00:00:45.109
vector, cmath, and so on.

00:00:45.109 --> 00:00:48.799
Then we have the OpenCV includes which include the core functionality,

00:00:48.799 --> 00:00:51.093
some visualization staff, image processing,

00:00:51.094 --> 00:00:54.980
and also everything related to features and descriptives.

00:00:54.979 --> 00:00:58.564
Then we have our own data structures which we will look at shortly.

00:00:58.564 --> 00:01:01.564
Also the matching2D.hpp include,

00:01:01.564 --> 00:01:04.144
this basically everything associated with

00:01:04.144 --> 00:01:07.429
camera-based processing loop such as feature detection,

00:01:07.430 --> 00:01:10.805
descriptor, extraction, matching, etc.

00:01:10.805 --> 00:01:13.100
Then in the main program.

00:01:13.099 --> 00:01:17.854
In the main program the first part is about initializing variables and data structures.

00:01:17.855 --> 00:01:21.770
Here's the location of the images on your hard disc currently

00:01:21.769 --> 00:01:26.814
it's a relative path which starts from where the executable is.

00:01:26.814 --> 00:01:31.219
Usually when you are using the virtual environment which is provided to you by Udacity,

00:01:31.219 --> 00:01:33.560
you don't have to change this variable here.

00:01:33.560 --> 00:01:38.225
But when you import this project into your own environment and it doesn't work,

00:01:38.224 --> 00:01:39.899
this might be a place to look for,

00:01:39.900 --> 00:01:43.844
where you need to make some adjustments so that this works correctly for you.

00:01:43.844 --> 00:01:47.900
Then comes the camera section which has the image-based path,

00:01:47.900 --> 00:01:51.905
the image prefix which is currently one of the KITTI sequences.

00:01:51.905 --> 00:01:54.185
Also the file type PNG.

00:01:54.185 --> 00:01:56.490
We have a start index and an end index.

00:01:56.489 --> 00:01:59.479
If you look into the overview on the left,

00:01:59.480 --> 00:02:03.814
we have an image folder here which basically contains all the images you will be using.

00:02:03.814 --> 00:02:05.209
Let's take a quick look.

00:02:05.209 --> 00:02:07.549
This is an image which you are already familiar with.

00:02:07.549 --> 00:02:12.710
It's the highway scenario which we have been using in quite a few examples up until now.

00:02:12.710 --> 00:02:15.349
We have a total of 10 images of this type.

00:02:15.349 --> 00:02:20.114
Let's go to the last one and observe the vehicle directly in front of you now.

00:02:20.115 --> 00:02:22.939
This is the last image. As you can see this is much closer now.

00:02:22.939 --> 00:02:25.960
So basically, a sequence of 10 images where

00:02:25.960 --> 00:02:29.629
the preceding vehicle slowly approaches the ego vehicle.

00:02:29.629 --> 00:02:32.419
That's the data which we will be looking at.

00:02:32.419 --> 00:02:34.444
Let's go back to the main file.

00:02:34.444 --> 00:02:36.169
Now what you can see here,

00:02:36.169 --> 00:02:40.484
in the next part some miscellaneous variables,

00:02:40.485 --> 00:02:43.010
the data buffer size and this one is very important for

00:02:43.009 --> 00:02:46.599
the first exercise where you will be implementing a ring buffer.

00:02:46.599 --> 00:02:48.960
This ring buffer will hold images,

00:02:48.960 --> 00:02:50.370
will hold key point matches,

00:02:50.370 --> 00:02:52.080
will later hold LiDAR points,

00:02:52.080 --> 00:02:57.260
and it's going to be stored in a in a dynamic list in a vector of type dataframe.

00:02:57.259 --> 00:02:58.849
Let's look at dataframe now.

00:02:58.849 --> 00:03:00.289
DataFrame is contained in

00:03:00.289 --> 00:03:04.039
dataStructures.h on the left-hand side let's open this one up here.

00:03:04.039 --> 00:03:06.620
As you can see here a DataFrame it's going to

00:03:06.620 --> 00:03:10.655
be small for now but we will expand on it later.

00:03:10.655 --> 00:03:13.340
For now it contains the camera image,

00:03:13.340 --> 00:03:14.870
it contains a list of key points,

00:03:14.870 --> 00:03:19.895
it contains descriptors which we compute from the key point,

00:03:19.895 --> 00:03:23.750
and also it contains a set of matches which connect this current data frame

00:03:23.750 --> 00:03:27.800
to the next DataFrame which comes later in time.

00:03:27.800 --> 00:03:30.620
Let's go back to the main file here.

00:03:30.620 --> 00:03:35.569
Once we did all this, also we have a visualization flag which you can switch on and off

00:03:35.569 --> 00:03:40.894
to have some intermediate views of the results down below.

00:03:40.895 --> 00:03:43.745
Then comes the main loop over all the images.

00:03:43.745 --> 00:03:48.770
This starts at index zero and goes up until the last index,

00:03:48.770 --> 00:03:51.284
it's the imageEndIndex here,

00:03:51.284 --> 00:03:52.814
nine in our case.

00:03:52.814 --> 00:03:57.604
In the first part what we do here is we load all the images into this buffer.

00:03:57.604 --> 00:04:02.988
The first part we assemble the filenames for the current index so that IMG full filename

00:04:02.989 --> 00:04:05.300
gives you the absolute path including

00:04:05.300 --> 00:04:08.735
the file name and the ending of the image we will be loading.

00:04:08.735 --> 00:04:12.455
We'll load this using the imread function down below here,

00:04:12.455 --> 00:04:14.900
providing it with a full filename.

00:04:14.900 --> 00:04:18.185
Then we are converting it into color format.

00:04:18.185 --> 00:04:24.670
Currently we are using only grayscale images for this KITTI folder.

00:04:26.600 --> 00:04:31.370
Currently we are only using grayscale images but should you be using color images,

00:04:31.370 --> 00:04:36.290
this would be the conversion function to move from BGR format to grayscale.

00:04:36.290 --> 00:04:39.650
So then we have the first student assignment which will have

00:04:39.649 --> 00:04:43.639
a detailed introduction later on in this documentation here.

00:04:43.639 --> 00:04:46.264
For now, it is suffice to say that

00:04:46.264 --> 00:04:50.029
the only thing which we are doing here in this part is,

00:04:50.029 --> 00:04:53.014
we are pushing the current image into

00:04:53.014 --> 00:04:57.729
a data frame buffer and add the data frame buffer to the end of the vector.

00:04:57.730 --> 00:05:01.675
So instantiate the DataFrame,

00:05:01.675 --> 00:05:06.365
add the image to it and then push this data frame into the dynamic list.

00:05:06.365 --> 00:05:07.715
That's everything that's done here.

00:05:07.714 --> 00:05:11.329
That's not very optimal as you will see in the first assignment.

00:05:11.329 --> 00:05:13.519
So then we have completed

00:05:13.519 --> 00:05:17.060
the first part which is loading a single image into the buffer. This is done.

00:05:17.060 --> 00:05:21.439
Then we move on to the next part which is about detecting image key points.

00:05:21.439 --> 00:05:24.055
Currently in the base implementation of the student version,

00:05:24.055 --> 00:05:26.290
this is done using the Shi-Tomasi method.

00:05:26.290 --> 00:05:30.110
As you can see here I'm using a string which is called detector type,

00:05:30.110 --> 00:05:35.310
to later on select which key point detection method which we will be using.

00:05:35.310 --> 00:05:39.060
Currently I have only Shi-Tomasi switched on.

00:05:39.079 --> 00:05:43.430
I think it's the second assignment task MP.2.

00:05:43.430 --> 00:05:48.004
This task would be to use HARRIS, FAST, BRISK,

00:05:48.004 --> 00:05:53.855
ORB and several other detectors to use in addition to the Shi-Tomasi method.

00:05:53.855 --> 00:06:02.855
You will choose the respective method by selecting this string here.

00:06:02.855 --> 00:06:04.189
So that's the second assignment.

00:06:04.189 --> 00:06:06.560
After calling to this function,

00:06:06.560 --> 00:06:11.930
we have a set of key points for each image stored in this key point variable here.

00:06:11.930 --> 00:06:14.435
I forgot to mention this just now,

00:06:14.435 --> 00:06:19.384
this is also a vector which takes an OpenCV datatype called key point.

00:06:19.384 --> 00:06:21.529
This holds for each image.

00:06:21.529 --> 00:06:25.309
This holds the set of all the key points detected in the image.

00:06:25.310 --> 00:06:29.555
Then we move on to the next part which is task MP.3.

00:06:29.555 --> 00:06:33.889
Sometimes it makes sense to focus only on a small selection of key points.

00:06:33.889 --> 00:06:36.409
For this MidTermProject, we only want to keep

00:06:36.410 --> 00:06:40.025
key points which are directly located on the preceding vehicle.

00:06:40.024 --> 00:06:43.189
Of course there are key points everywhere on the road surface,

00:06:43.189 --> 00:06:45.040
on the vehicles around,

00:06:45.040 --> 00:06:46.950
around our own ego vehicle,

00:06:46.949 --> 00:06:49.875
on the bridge which is crossing over the highway.

00:06:49.875 --> 00:06:51.560
So everywhere are key points,

00:06:51.560 --> 00:06:53.930
but what we want to do is we want to only

00:06:53.930 --> 00:06:57.894
evaluate the key points which are directly located on the vehicle in front of us.

00:06:57.894 --> 00:06:59.859
In order to do so at least roughly,

00:06:59.860 --> 00:07:02.764
we want to define a data structure which is called

00:07:02.764 --> 00:07:06.500
Rect datatype containing opens in the OpenCV.

00:07:06.500 --> 00:07:11.329
This is a setting which for sequence of nine images,

00:07:11.329 --> 00:07:17.750
which defines a rectangle which always encloses the directly preceding vehicle.

00:07:17.750 --> 00:07:20.420
So this is another task for you.

00:07:20.420 --> 00:07:26.670
Filter out all the key points which are located outside of this bounding rectangle,

00:07:26.670 --> 00:07:28.574
and only keep the ones within.

00:07:28.574 --> 00:07:32.735
This comes later in the detail assignment to task MP.3.

00:07:32.735 --> 00:07:37.655
Now once you have filtered the key points on the vehicle in front of us,

00:07:37.654 --> 00:07:39.979
sometimes again it makes sense

00:07:39.980 --> 00:07:43.310
to limit the number of key points to let's say a number of 50,

00:07:43.310 --> 00:07:48.680
if we have so many matches that visualization only gives us a blur of colors,

00:07:48.680 --> 00:07:51.185
where we cannot look at individual key points.

00:07:51.185 --> 00:07:55.204
So if you have too many key points and you want to look at them more closely,

00:07:55.204 --> 00:07:57.259
you can switch on this flag here.

00:07:57.259 --> 00:07:59.449
Let's say switch this to 20,

00:07:59.449 --> 00:08:02.564
or 30, or any number you like.

00:08:02.564 --> 00:08:05.600
Then basically the number of key points which the detector

00:08:05.600 --> 00:08:08.915
found is lowered to the number indicated here.

00:08:08.915 --> 00:08:12.395
But this is only for debugging purposes, for tutorial purposes.

00:08:12.394 --> 00:08:16.789
This should not be switched on in a running version of the code where it's about getting

00:08:16.790 --> 00:08:21.540
as many key points as we can on the objects of interest to us.

00:08:21.540 --> 00:08:31.220
So switch this on only for scenarios where we want to look closely at certain key points,

00:08:31.220 --> 00:08:35.450
when the visualization is too overload it to be of meaning to us.

00:08:35.450 --> 00:08:39.500
Once this is complete what we do here is we are simply pushing

00:08:39.500 --> 00:08:44.779
the key points we found into the last element contained in the data buffer.

00:08:44.779 --> 00:08:51.095
Once this is done, part two is ready detect key points done.

00:08:51.095 --> 00:08:53.210
Let's move on to the next part which is about

00:08:53.210 --> 00:08:56.855
extracting the key point descriptors for all the key points.

00:08:56.855 --> 00:09:00.154
So this is also the next assignment task MP.4.

00:09:00.154 --> 00:09:04.970
We have one descriptor already implemented as a BRISK descriptor.

00:09:04.970 --> 00:09:07.024
In addition to the brisk descriptor,

00:09:07.024 --> 00:09:10.194
you're supposed to implement BRIEF, ORB,

00:09:10.195 --> 00:09:14.495
FREAK, AKAZE, and the SIFT descriptor also.

00:09:14.495 --> 00:09:16.730
But this is performed within

00:09:16.730 --> 00:09:20.420
the function desk key points which we will look at very shortly.

00:09:20.419 --> 00:09:24.349
So also again, this should be selectable using

00:09:24.350 --> 00:09:29.899
this string called descriptor type here similar to the detector type above.

00:09:29.899 --> 00:09:33.470
So once descriptor extraction is finished,

00:09:33.470 --> 00:09:37.160
descriptors are added also to the data frame structure,

00:09:37.159 --> 00:09:40.294
and the last element in our data buffer respectively.

00:09:40.294 --> 00:09:44.974
Then we can move on to the next part which is about matching key points.

00:09:44.975 --> 00:09:49.370
This part is only entered into once the data buffer size exceeds a single element,

00:09:49.370 --> 00:09:53.190
which makes sense because we want to match images between two features or two.

00:09:53.190 --> 00:09:55.725
We want to match key points between two images.

00:09:55.725 --> 00:09:57.930
If you only have one, this simply doesn't work,

00:09:57.929 --> 00:09:59.870
so we need at least the data buffer size of

00:09:59.870 --> 00:10:03.289
two to be able to match key points descriptors.

00:10:03.289 --> 00:10:07.759
Now the data structure which contains other matches is also a vector which

00:10:07.759 --> 00:10:12.379
contains DMatches which is an OpenCV data type you might want to look into more closely.

00:10:12.379 --> 00:10:17.554
But this is the result structure which we will fill with our matching approach.

00:10:17.554 --> 00:10:21.709
So currently, we can select three different things here.

00:10:21.710 --> 00:10:23.269
We have the matcher type which

00:10:23.269 --> 00:10:27.620
lets you choose between Brute Force and FLANN-based matching.

00:10:27.620 --> 00:10:31.384
Currently, in the student version only a brute force matching is implemented.

00:10:31.384 --> 00:10:34.850
Then, you can select the type of descriptor which you have;

00:10:34.850 --> 00:10:38.855
is it a binary descriptors such as brisk or such as orb,

00:10:38.855 --> 00:10:43.789
or is it histogram of oriented gradients based descriptors such as sift,

00:10:43.789 --> 00:10:45.365
this one as indicated here.

00:10:45.365 --> 00:10:51.769
Then you can also choose the type of nearest neighbor approach you want to implement.

00:10:51.769 --> 00:10:57.319
Is it the nearest neighbor approach where only the closest match is selected,

00:10:57.320 --> 00:11:01.775
or is it the k-Nearest Neighbor approach where you keep the k nearest neighbors?

00:11:01.774 --> 00:11:03.049
If k is at the two,

00:11:03.049 --> 00:11:07.745
you would get the best two matches for each descriptive pair.

00:11:07.745 --> 00:11:11.810
Okay. Once we have selected those strings here,

00:11:11.809 --> 00:11:13.969
the actual matching is called.

00:11:13.970 --> 00:11:16.790
The function match descriptors is something which we will look at shortly.

00:11:16.789 --> 00:11:19.865
It's also in the matching 2D function.

00:11:19.865 --> 00:11:26.330
It takes as input a set of keypoints for the previous dataframe,

00:11:26.330 --> 00:11:27.770
for the current dataframe,

00:11:27.769 --> 00:11:29.794
also their respective descriptors here,

00:11:29.794 --> 00:11:34.024
and then all the configuration strings which we just talked about now.

00:11:34.024 --> 00:11:41.539
The output is the matches vector which is defined in line 143, this one here.

00:11:41.539 --> 00:11:43.324
Once this function is complete,

00:11:43.325 --> 00:11:46.910
we have a set of matches between the current dataframe and

00:11:46.909 --> 00:11:52.759
the previous dataframe which will then be visualized in the last part.

00:11:52.759 --> 00:11:56.240
So this basically is what's in the midterm project for now.

00:11:56.240 --> 00:12:02.735
We have the last line here which adds the matches to the last data buffer element here,

00:12:02.735 --> 00:12:06.185
and then we only have the visualization down here below.

00:12:06.184 --> 00:12:10.139
Let's run the code to see how this looks.

00:12:15.610 --> 00:12:19.879
So what you can see here is a pretty colorful image of

00:12:19.879 --> 00:12:24.169
the first frame and the second frame of the image sequence.

00:12:24.169 --> 00:12:27.379
All those colored lines are matched keypoint pairs

00:12:27.379 --> 00:12:32.414
using the Shi-Tomasi detector and the brisk descriptor.

00:12:32.414 --> 00:12:38.230
As you can see, there are some lines which span the image diagonally.

00:12:38.230 --> 00:12:41.590
These ones are false matches where we have

00:12:41.590 --> 00:12:45.205
matched two keypoint pairs which obviously do not belong to each other,

00:12:45.205 --> 00:12:47.259
and matches which are in

00:12:47.259 --> 00:12:52.240
a horizontal line even though we cannot see it quite clearly here,

00:12:52.240 --> 00:12:55.870
indicate that at least the respective match is in

00:12:55.870 --> 00:13:01.185
the same row as the feature on the left side here.

00:13:01.184 --> 00:13:03.739
So for visualization purposes,

00:13:03.740 --> 00:13:06.695
now let's switch on the feature,

00:13:06.695 --> 00:13:14.360
and let's limit the number of keypoints to the best 50 matches here.

00:13:14.360 --> 00:13:15.935
Let's run it again,

00:13:15.934 --> 00:13:20.699
look at the code, look at the output.

00:13:23.769 --> 00:13:27.889
As you can see, it's much better visible now.

00:13:27.889 --> 00:13:29.480
For example, here again,

00:13:29.480 --> 00:13:33.470
we have this branch on the left-hand side.

00:13:33.470 --> 00:13:35.450
This is the tree branch here,

00:13:35.450 --> 00:13:40.220
and it's matched to the same position in the right image.

00:13:40.220 --> 00:13:44.899
Also we have a match on the right tail light and this match,

00:13:44.899 --> 00:13:47.524
if you follow the brown line to the right,

00:13:47.524 --> 00:13:51.334
this matches also to the right tail lights here.

00:13:51.335 --> 00:13:53.360
On the other hand, this one here,

00:13:53.360 --> 00:13:54.605
the left tail light,

00:13:54.605 --> 00:13:55.940
if you follow the blue line,

00:13:55.940 --> 00:14:00.770
this matches to the left tail light over the vehicle on the left.

00:14:00.769 --> 00:14:04.189
So this is a false match and this is something we do not want to keep

00:14:04.190 --> 00:14:08.000
because this would give us a false estimate of the time to collision obviously.

00:14:08.000 --> 00:14:10.835
Okay. So the main function here,

00:14:10.835 --> 00:14:14.060
now let's look into the matching

00:14:14.059 --> 00:14:18.125
2Dstudent.cpp where all the image processing components are.

00:14:18.125 --> 00:14:21.200
By the way, if you press space,

00:14:21.200 --> 00:14:24.230
the image sequence progresses through

00:14:24.230 --> 00:14:28.085
all the nine images or the 10 images which we have in the sequence.

00:14:28.085 --> 00:14:31.865
So inside matching 2D student.cpp.

00:14:31.865 --> 00:14:33.950
What we have here are several functions.

00:14:33.950 --> 00:14:36.560
The first one is matched descriptors.

00:14:36.559 --> 00:14:39.364
This is the one we're currently Brute force matching,

00:14:39.365 --> 00:14:42.904
is implemented using this structure here.

00:14:42.904 --> 00:14:46.039
It's a Brute force matching method we select from

00:14:46.039 --> 00:14:50.480
the openCV using the hamming distance because it's a binary descriptor.

00:14:50.480 --> 00:14:54.980
This would be the position where you should implement the FLANN-based matching method.

00:14:54.980 --> 00:14:58.639
Here is the nearest neighbor selector which is currently used,

00:14:58.639 --> 00:15:01.220
and here would be the k-nearest neighbor

00:15:01.220 --> 00:15:04.430
selector which you should implement in one of the tasks.

00:15:04.429 --> 00:15:06.109
So this is the matching part.

00:15:06.110 --> 00:15:11.764
Then here we have the descriptor extraction part, descriptor keypoints.

00:15:11.764 --> 00:15:14.870
Currently, we have the brisk descriptor which is used

00:15:14.870 --> 00:15:18.889
here drawing from the OpenCV implementation,

00:15:18.889 --> 00:15:26.100
which cause on creating the extractor function taking some parameters here.

00:15:30.759 --> 00:15:36.875
After creating this data structure and storing it into the extractor variable here,

00:15:36.875 --> 00:15:40.370
we are then calling the compute function on it,

00:15:40.370 --> 00:15:43.685
and compute basically gives you the set of

00:15:43.684 --> 00:15:47.629
keypoint descriptors which are then stored in this variable here.

00:15:47.629 --> 00:15:50.509
Also we are measuring how long it takes

00:15:50.509 --> 00:15:54.860
to perform the actual descriptor extraction here in milliseconds.

00:15:54.860 --> 00:15:58.789
Then lastly, the last function here is that keypoint Shi-Tomasi.

00:15:58.789 --> 00:16:00.529
You have seen this function before,

00:16:00.529 --> 00:16:02.674
I will not go into too many details on this.

00:16:02.674 --> 00:16:05.479
Suffice it to say we have a set of variables which

00:16:05.480 --> 00:16:10.504
basically define the number of keypoints which the function will provide to you.

00:16:10.504 --> 00:16:14.975
It also sets the quality level and some other parameters.

00:16:14.975 --> 00:16:18.004
We call the function which is called good features to track,

00:16:18.004 --> 00:16:20.389
and then we store the results in

00:16:20.389 --> 00:16:23.059
a vector and measure also how long it

00:16:23.059 --> 00:16:26.329
takes to compute to compute the Shi-Tomasi keypoints.

00:16:26.330 --> 00:16:28.985
So now you have an understanding of a coding framework,

00:16:28.985 --> 00:16:30.919
and once you have completed the Midterm project,

00:16:30.919 --> 00:16:33.169
it will be performing much better and you will be

00:16:33.169 --> 00:16:35.569
able to use it in the final project of this course.

00:16:35.570 --> 00:16:37.220
But before we get ahead of ourselves,

00:16:37.220 --> 00:16:39.840
let's take a look at your first task.

