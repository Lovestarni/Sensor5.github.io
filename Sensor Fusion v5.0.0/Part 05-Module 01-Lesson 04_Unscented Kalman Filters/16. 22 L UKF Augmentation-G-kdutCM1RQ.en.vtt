WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.990
You implemented the sigma point generation in C plus plus.

00:00:03.990 --> 00:00:06.060
That's very good, because it's

00:00:06.059 --> 00:00:09.839
a great preparation for the project at the end of this module.

00:00:09.839 --> 00:00:12.719
Now you know how to represent the uncertainty of

00:00:12.720 --> 00:00:15.825
the posterior state estimation with sigma points,

00:00:15.824 --> 00:00:19.335
and we can put the sigma points into the process function.

00:00:19.335 --> 00:00:25.890
But wait a second, the process function also considers the process noise vector mu k,

00:00:25.890 --> 00:00:28.905
and this also has a nonlinear effect.

00:00:28.905 --> 00:00:33.329
Fortunately, the UKF provides a super easy way,

00:00:33.329 --> 00:00:36.625
to handle nonlinear process noise effects,

00:00:36.625 --> 00:00:39.950
and this is what I want to show you in this video.

00:00:39.950 --> 00:00:42.815
Let's look into this process noise again.

00:00:42.814 --> 00:00:45.734
This is the complete process model,

00:00:45.734 --> 00:00:48.445
considering the process noise vector.

00:00:48.445 --> 00:00:51.320
Looking at this, some of you might think,

00:00:51.320 --> 00:00:57.789
wait a second didn't we say this type of vector is the process vector mu k?

00:00:57.789 --> 00:01:00.814
Yes, you're absolutely right.

00:01:00.814 --> 00:01:03.619
In the last lesson about the common filter,

00:01:03.619 --> 00:01:06.265
this was what we called mu k,

00:01:06.265 --> 00:01:08.450
and that made a big difference because,

00:01:08.450 --> 00:01:13.025
you had to calculate this big process noise covariance matrix Q.

00:01:13.025 --> 00:01:18.500
The thing is, if you look into literature and you find this process noise vector mu k,

00:01:18.500 --> 00:01:21.769
you really have to be careful what the author's mean.

00:01:21.769 --> 00:01:24.259
Sometimes, they mean this vector.

00:01:24.260 --> 00:01:26.419
Each row of this vector,

00:01:26.418 --> 00:01:29.269
tells the influence of the process noise,

00:01:29.269 --> 00:01:33.155
on a specific state between k and k plus one.

00:01:33.155 --> 00:01:36.620
So, it has the same dimension as the state vector,

00:01:36.620 --> 00:01:39.490
and it depends on delta t. Mostly,

00:01:39.489 --> 00:01:43.609
this is the case when you read about the standard linear common filter.

00:01:43.609 --> 00:01:46.429
But sometimes the author's mean this vector.

00:01:46.430 --> 00:01:49.580
This is something much simpler because this vector just

00:01:49.579 --> 00:01:52.789
lists the individual independent noise processes.

00:01:52.790 --> 00:01:54.920
This vector doesn't say anything about

00:01:54.920 --> 00:01:57.965
the effect of these noise processes on the state vector,

00:01:57.965 --> 00:02:01.475
and it does not depend on delta t. Usually,

00:02:01.474 --> 00:02:05.184
this is what authors mean when they write about the unscented common filter.

00:02:05.185 --> 00:02:09.680
This is you confuse me a lot when I started getting into common filters.

00:02:09.680 --> 00:02:11.885
So, in this lesson about the UKF,

00:02:11.884 --> 00:02:13.174
when I mentioned UK,

00:02:13.175 --> 00:02:16.055
I'm always talking about this process noise vector,

00:02:16.055 --> 00:02:18.049
and this is a good thing because,

00:02:18.049 --> 00:02:20.800
the covariance matrix Q of this process noise,

00:02:20.800 --> 00:02:22.945
is much simpler in this case.

00:02:22.944 --> 00:02:24.780
To write down this matrix,

00:02:24.780 --> 00:02:27.229
you just need to know the stochastic properties of

00:02:27.229 --> 00:02:30.389
these noise processes which we already defined.

00:02:30.389 --> 00:02:33.154
Then Q is given by this matrix.

00:02:33.155 --> 00:02:36.979
This matrix contains the variances of the noise processes.

00:02:36.979 --> 00:02:39.379
The co-variances here is zero,

00:02:39.379 --> 00:02:41.974
because the longitudinal acceleration noise

00:02:41.974 --> 00:02:45.109
and the yaw acceleration noise are considered independent.

00:02:45.110 --> 00:02:48.935
Now, we clarified what this process noise vector is,

00:02:48.935 --> 00:02:52.295
and what its covariance matrix Q looks like.

00:02:52.294 --> 00:02:53.839
What I want to show you now,

00:02:53.840 --> 00:02:59.349
is how you can represent the uncertainty of the covariance matrix Q, with sigma points.

00:02:59.349 --> 00:03:01.835
The solution is called augmentation.

00:03:01.835 --> 00:03:07.099
This is an overview on how we generated sigma points without considering Q.

00:03:07.099 --> 00:03:08.689
What we will do now is,

00:03:08.689 --> 00:03:10.430
build the augmented state,

00:03:10.430 --> 00:03:13.849
where we add the noise vector to the state vector,

00:03:13.849 --> 00:03:16.294
I call this state XA.

00:03:16.294 --> 00:03:19.879
The augmented state dimension in our case is seven.

00:03:19.879 --> 00:03:23.224
This also means we need 15 sigma points now.

00:03:23.224 --> 00:03:25.609
The additional four sigma points will be

00:03:25.610 --> 00:03:29.240
representing the uncertainty caused by the process noise.

00:03:29.240 --> 00:03:33.215
We also build the augmented covariance matrix PA.

00:03:33.215 --> 00:03:37.055
This matrix has seven rows, and seven columns.

00:03:37.055 --> 00:03:42.034
You just add the matrix Q as a lower right block,

00:03:42.034 --> 00:03:45.115
and fill up the remaining fields with a zero.

00:03:45.115 --> 00:03:50.765
Finally, the way to generate the 15 sigma points is exactly the same as before.

00:03:50.764 --> 00:03:53.854
Just use the augmented elements as input.

00:03:53.854 --> 00:03:55.729
In the next quiz,

00:03:55.729 --> 00:03:58.584
you will adapt your sigma point generation function,

00:03:58.585 --> 00:04:00.599
to the augmented state.

