WEBVTT
Kind: captions
Language: zh-CN

00:00:00.510 --> 00:00:04.919
恭喜你！利用扩展和无损卡尔曼滤波器

00:00:04.919 --> 00:00:10.000
把激光雷达和雷达测量值融合到一起 是了不起的成就

00:00:10.000 --> 00:00:15.140
这是我们在梅赛德斯-奔驰的传感器融合团队的核心方法

00:00:15.140 --> 00:00:18.920
- 我们考虑一下 传感器融合是如何融入整个无人驾驶车

00:00:18.920 --> 00:00:20.620
技术栈的

00:00:20.620 --> 00:00:25.310
首先 传感器记录数据 接下来是预处理步骤

00:00:25.310 --> 00:00:30.240
如使用聚类算法或更复杂的方案来检测对象假设

00:00:30.239 --> 00:00:34.489
最后 我们可以把这些假设作为对象级别融合的输入

00:00:35.579 --> 00:00:37.170
- 在传感器融合模块中

00:00:37.170 --> 00:00:40.980
我们把所有数据整合到了一起 就像你在本课中做的一样

00:00:40.979 --> 00:00:43.269
这并不是唯一的实现流程

00:00:43.270 --> 00:00:47.660
每个处理步骤中都可以应用传感器融合

00:00:47.659 --> 00:00:51.819
- 保证了对周围世界的一致表述后

00:00:51.820 --> 00:00:56.670
我们就可以把结果传递给无人驾驶车系统的

00:00:56.670 --> 00:01:01.250
下游环节 不同团队可以使用这些数据来定位车辆

00:01:01.250 --> 00:01:04.920
控制车辆以及规划车辆路径

00:01:04.920 --> 00:01:08.299
- 我们还需要考虑 真实世界和我们

00:01:08.299 --> 00:01:10.359
目前所学内容有哪些不同之处

00:01:10.359 --> 00:01:12.060
例如 在本课中

00:01:12.060 --> 00:01:15.030
我们假设只追踪一个对象

00:01:15.030 --> 00:01:17.680
但如果街道上有多个对象呢？

00:01:17.680 --> 00:01:21.210
这时候你必须确定每个测量值和具体车辆的对应关系

00:01:21.209 --> 00:01:23.299
传感器融合会更加困难

00:01:23.299 --> 00:01:26.709
- 但是 你在本课中学到的核心方法是

00:01:26.709 --> 00:01:31.429
处理这些复杂场景所需的扩展传感器融合模块的关键所在

00:01:31.430 --> 00:01:35.110
- 传感器融合是无人驾驶车系统关键部分

00:01:35.109 --> 00:01:37.909
系统的其他部分都依赖于这部分的数据

00:01:37.909 --> 00:01:42.039
-  在后面的模块里 你会了解无人驾驶车系统

00:01:42.040 --> 00:01:45.690
的其他部分是如何使用传感器数据实现具体功能的

00:01:45.689 --> 00:01:47.530
下面是最后一个传感器融合实战项目 祝你好运！

