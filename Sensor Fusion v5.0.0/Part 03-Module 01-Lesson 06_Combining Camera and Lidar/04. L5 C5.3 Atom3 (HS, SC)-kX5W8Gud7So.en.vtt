WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.180
Now the goal we are trying to reach here is this,

00:00:02.180 --> 00:00:05.595
imagine a region of interest around a target vehicle in the left lane.

00:00:05.594 --> 00:00:08.205
The sensor is here, we're looking at this vehicle.

00:00:08.205 --> 00:00:11.189
While this region might be perfectly around the entire vehicle,

00:00:11.189 --> 00:00:13.274
there will be areas in which other objects are

00:00:13.275 --> 00:00:15.780
visible from the perspective of the sensors.

00:00:15.779 --> 00:00:20.369
Such as, for example, the vehicle or the front of it and the group in step three

00:00:20.370 --> 00:00:23.070
defines from this object and this object will

00:00:23.070 --> 00:00:26.024
be associated with this target vehicle, which is not what we want.

00:00:26.024 --> 00:00:29.070
So one very effective and quick way to avoid this would be to simply

00:00:29.070 --> 00:00:32.804
count the number of regions of interest by which lidar points are enclosed.

00:00:32.804 --> 00:00:35.594
In case there is only a single region of interest, does not trouble at all.

00:00:35.594 --> 00:00:37.335
In case of multiple ROIs,

00:00:37.335 --> 00:00:41.700
we simply ignore the lidar point and do not associate this with any other RI.

00:00:41.700 --> 00:00:43.260
So we will lose the point, that's true.

00:00:43.259 --> 00:00:46.729
But on the other hand, what we get is the clean 3D object in turn,

00:00:46.729 --> 00:00:49.954
which is not a mixture of several objects as it would be now.

00:00:49.954 --> 00:00:52.864
So let's take a look in the code how this can be done.

00:00:52.865 --> 00:00:55.700
So let's take a look at how the clustering of lidar points

00:00:55.700 --> 00:00:58.775
using bounding boxes from yellow object detection can be done.

00:00:58.774 --> 00:01:00.439
So the first thing we do here in

00:01:00.439 --> 00:01:04.295
the run clustering method which is what you would call in this example,

00:01:04.295 --> 00:01:08.225
as we read the lidar points from a file, from this one here.

00:01:08.224 --> 00:01:12.379
Then we read the bounding boxes from another file which is this one here.

00:01:12.379 --> 00:01:16.159
Both files have been generated by the code of the final project,

00:01:16.159 --> 00:01:20.629
which I have used to process the image sequence which you already know

00:01:20.629 --> 00:01:25.269
from the highway scenario where the vehicle in front of us is slowly approaching.

00:01:25.269 --> 00:01:28.579
We have vehicles in the left lane and a truck in the right lane.

00:01:28.579 --> 00:01:30.754
So that's the data we see here.

00:01:30.754 --> 00:01:33.949
Once we have loaded lidar points as well as the bounding boxes,

00:01:33.950 --> 00:01:36.620
we can call the function cluster lidar with ROI.

00:01:36.620 --> 00:01:39.785
The idea here is we input bounding boxes which are

00:01:39.784 --> 00:01:44.340
rectangles regions of interest in the image and we input lidar points,

00:01:44.340 --> 00:01:47.870
then we project the lidar points into the image and we

00:01:47.870 --> 00:01:51.920
group all the lidar points belonging to a specific bounding box to this bounding box.

00:01:51.920 --> 00:01:53.600
So once this has been done here,

00:01:53.599 --> 00:01:57.239
assuming we have lidar points everywhere in the scene,

00:01:57.239 --> 00:01:59.959
every bounding box in this vector here will

00:01:59.959 --> 00:02:04.129
contain specific lidar points which belong to this bounding box.

00:02:04.129 --> 00:02:08.509
We can then display those bounding boxes and the lidar points by looping

00:02:08.509 --> 00:02:11.120
all the bounding boxes and should there be

00:02:11.120 --> 00:02:14.980
any lidar points within the bounding box which is verified by this line here,

00:02:14.979 --> 00:02:19.944
then we simply display this isolated 3D object in a top view perspective.

00:02:19.944 --> 00:02:25.174
The idea here is now to avoid that we have this mixing of 3D objects,

00:02:25.175 --> 00:02:29.930
which happens when lidar point falls into two different bounding boxes.

00:02:29.930 --> 00:02:31.580
So we have this example,

00:02:31.580 --> 00:02:34.505
we have discussed it in section.

00:02:34.504 --> 00:02:38.419
If there are any points in the left lane vehicle which are enclosed by

00:02:38.419 --> 00:02:42.709
a bounding box which belongs to the first car and also maybe to the second car,

00:02:42.710 --> 00:02:47.780
those lidar points now will not be displayed any longer to avoid those grouping errors.

00:02:47.780 --> 00:02:49.640
Let's see if this works and how this can be done.

00:02:49.639 --> 00:02:52.939
So let's take a look at cluster lidar with ROI.

00:02:52.939 --> 00:02:55.234
This is what's called here.

00:02:55.235 --> 00:02:56.720
We have a vector of bounding boxes,

00:02:56.719 --> 00:02:58.310
we have a vector of lidar points,

00:02:58.310 --> 00:03:02.254
and the first thing we do is we load the calibration data

00:03:02.254 --> 00:03:07.269
into OpenCV matrices and we do so with the same code which you have used before,

00:03:07.270 --> 00:03:10.640
the only difference here is for convenience to be a bit shorter,

00:03:10.639 --> 00:03:15.049
I have put those lengthy lines where we simply

00:03:15.050 --> 00:03:17.030
copy and paste the calibration information from

00:03:17.030 --> 00:03:19.520
the kitty file into the respective variables,

00:03:19.520 --> 00:03:21.425
into this function here locked integration data.

00:03:21.425 --> 00:03:24.064
Just makes the code a little bit more legible.

00:03:24.064 --> 00:03:28.405
But look it up if you have doubts on how this actually has been performed.

00:03:28.405 --> 00:03:30.229
Okay. Let's continue for now.

00:03:30.229 --> 00:03:31.519
Once we leave this line here,

00:03:31.520 --> 00:03:34.040
we have the calibration matrix for

00:03:34.039 --> 00:03:38.224
intrinsic data for rectifying which [inaudible] left and right camera,

00:03:38.224 --> 00:03:42.294
as well as for extrinsic data which is rotation and translation,

00:03:42.294 --> 00:03:48.564
which we will use to project lidar 3D point in space onto the image plane.

00:03:48.564 --> 00:03:51.569
This is what's done here inside this for loop.

00:03:51.569 --> 00:03:54.724
So again, it's the same code that you have already seen before.

00:03:54.724 --> 00:03:57.394
We have a point in space,

00:03:57.395 --> 00:04:00.620
capital X which is inhomogeneous coordinates which

00:04:00.620 --> 00:04:04.039
we can simply create by adding a one here in the fourth component.

00:04:04.039 --> 00:04:08.449
Then we project the lidar point onto the image plane into

00:04:08.449 --> 00:04:13.639
the camera and the coordinates PT.x and PT.y,

00:04:13.639 --> 00:04:18.349
those are pixel coordinates which we received after reconverting

00:04:18.350 --> 00:04:21.200
the homogeneous coordinates in capital Y

00:04:21.199 --> 00:04:26.319
into Euclidean coordinates by simply dividing them by the third component here.

00:04:26.319 --> 00:04:31.969
Okay. Now what's happening here is we assume that not all the points at

00:04:31.970 --> 00:04:34.490
the outer edges of our bounding box

00:04:34.490 --> 00:04:37.730
belong to the vehicle which we actually want to observe.

00:04:37.730 --> 00:04:40.730
This is why we are going to resize this bounding box.

00:04:40.730 --> 00:04:42.910
We're going to make it a little bit smaller,

00:04:42.910 --> 00:04:48.230
and the shrinking factor can be set to any value between zero and one.

00:04:48.230 --> 00:04:54.715
Zero would be the original size and one would be a shrinking by a 100 percent.

00:04:54.714 --> 00:04:57.079
But now we are shrinking it by 10 percent only.

00:04:57.079 --> 00:05:00.079
So we're going to create a new bounding box which is

00:05:00.079 --> 00:05:03.949
called smallerBox and this smallerBox is going to shrink

00:05:03.949 --> 00:05:07.594
each bounding box which we get by iterating

00:05:07.595 --> 00:05:11.510
over this vector of bounding boxes which we have passed to the function here.

00:05:11.509 --> 00:05:15.154
So this follow [inaudible] of all the bounding boxes in the current image.

00:05:15.154 --> 00:05:18.229
Then, we resize the bounding box by 10 percent,

00:05:18.230 --> 00:05:20.275
which is done by those lines here.

00:05:20.274 --> 00:05:25.394
Then in this line, we simply check if the smallerBox contains all the point.

00:05:25.394 --> 00:05:27.539
Is it within the bounding box?

00:05:27.540 --> 00:05:30.200
Now what we do here is we do not simply store away

00:05:30.199 --> 00:05:33.229
this point and label it as belonging to this bounding box,

00:05:33.230 --> 00:05:39.050
but what we do we push back this bounding box which is currently addressed

00:05:39.050 --> 00:05:45.064
by the iterator IT2 into a vector which is called enclosing boxes.

00:05:45.064 --> 00:05:47.329
If you look at the beginning of this function,

00:05:47.329 --> 00:05:49.099
we have this vector here,

00:05:49.100 --> 00:05:52.970
which is basically a vector which holds iterators over bounding boxes.

00:05:52.970 --> 00:05:55.595
So the idea is to check for

00:05:55.595 --> 00:05:59.810
each point whether it has been enclosed by one or multiple boxes.

00:05:59.810 --> 00:06:02.120
So if a point is being enclosed by multiple boxes,

00:06:02.120 --> 00:06:05.120
what we will find is when we get to this position here,

00:06:05.120 --> 00:06:09.280
this position is reached once we have looped over all the bounding boxes here.

00:06:09.279 --> 00:06:13.054
Let's say a point has been enclosed by three bounding boxes.

00:06:13.055 --> 00:06:16.040
We will have identified all those three bounding boxes by checking

00:06:16.040 --> 00:06:19.295
in this line whether each box contains the respective point,

00:06:19.295 --> 00:06:23.045
all three bounding box iterators would've been pushed into enclosing boxes.

00:06:23.045 --> 00:06:27.335
So the size card of enclosing boxes in my example would be three.

00:06:27.334 --> 00:06:30.680
If a key point has been enclosed by so many bounding boxes,

00:06:30.680 --> 00:06:34.009
we cannot say to which 3D object it actually belongs.

00:06:34.009 --> 00:06:36.769
So what we want to do now is we only want to keep

00:06:36.769 --> 00:06:40.714
those points who only have been enclosed by a single bounding box.

00:06:40.714 --> 00:06:45.019
In doing so, we will discard all points which are enclosed by several bounding boxes,

00:06:45.019 --> 00:06:47.629
so we will lose some points but as lidar is

00:06:47.629 --> 00:06:49.504
such a fantastic sensor giving us

00:06:49.504 --> 00:06:52.480
so many hundreds of thousands of lidar points for a scene,

00:06:52.480 --> 00:06:54.185
I think we can afford this.

00:06:54.185 --> 00:06:58.535
So we are for safety reasons to really get accurate 3D object boundaries.

00:06:58.535 --> 00:07:03.580
We're only keeping key points which have been enclosed by a single bounding box only.

00:07:03.579 --> 00:07:06.019
Once we get here, we say, okay,

00:07:06.019 --> 00:07:08.944
so the bounding box which we want to

00:07:08.944 --> 00:07:14.665
associate the lidar point to is the first in the vector because we only have one.

00:07:14.665 --> 00:07:18.650
Then we simply push back this lidar point which is

00:07:18.649 --> 00:07:22.339
denoted by the iterator IT1 into this bounding box.

00:07:22.339 --> 00:07:28.519
So after completing the loop over all key points and over all bounding boxes,

00:07:28.519 --> 00:07:34.250
we will have associated all the lidar points whose projection is

00:07:34.250 --> 00:07:36.394
only enclosed by a single bounding box

00:07:36.394 --> 00:07:40.159
and addressed and pushed it into the respective bounding boxes.

00:07:40.160 --> 00:07:41.870
So once we leave the function,

00:07:41.870 --> 00:07:46.774
we have a set of bounding boxes and associated key points and the key points associated

00:07:46.774 --> 00:07:51.875
are unique in the sense that they have not been enclosed by any other bounding box.

00:07:51.875 --> 00:07:54.180
Let's see if this works.

00:07:54.920 --> 00:08:00.250
So what you can see here is a top view visualization of the first object.

00:08:00.250 --> 00:08:03.430
If you look into the lesson in HTML file,

00:08:03.430 --> 00:08:05.829
you will find a screenshot which is similar to this one.

00:08:05.829 --> 00:08:07.689
But in this screenshot which you will find,

00:08:07.689 --> 00:08:11.079
there is an object in front of this object here,

00:08:11.079 --> 00:08:15.639
which has been caused by including lidar points which belong to several bounding inboxes.

00:08:15.639 --> 00:08:19.449
So what we can see here is a very clean object which has

00:08:19.449 --> 00:08:24.009
no other lidar points except the ones belonging to this specific vehicle year.

00:08:24.009 --> 00:08:25.329
If I pushed space,

00:08:25.329 --> 00:08:28.944
we're going to move on to the next bounding box which is the truck in the right lane,

00:08:28.944 --> 00:08:32.620
then we move on to this red truck in the left lane.

00:08:32.620 --> 00:08:35.560
As you can see here, there are some lidar points missing.

00:08:35.559 --> 00:08:40.089
There is a black area here with not a single lidar point present.

00:08:40.090 --> 00:08:44.269
Actually, those lidar points are the ones having been cut out by

00:08:44.269 --> 00:08:49.625
the overlap between the two bounding boxes in the original object detection in the image.

00:08:49.625 --> 00:08:52.250
So our code seems to work with it perfectly.

00:08:52.250 --> 00:08:56.779
We have a very clean cut out of all the lidar points belonging only to the red truck

00:08:56.779 --> 00:09:01.610
and still we have so many points on the truck especially here on the rear side,

00:09:01.610 --> 00:09:06.080
on the tail gauge that can very well afford to lose a few points here,

00:09:06.080 --> 00:09:08.420
which are caused by a bounding box overlap.

00:09:08.419 --> 00:09:13.264
Let's cycle through the code and through other bounding boxes until we reach the end.

00:09:13.264 --> 00:09:15.830
This is the vehicle directly in front of us and as you can

00:09:15.830 --> 00:09:18.830
see here at the far end of our scanning area,

00:09:18.830 --> 00:09:22.070
we actually did group another vehicle into this bonding box.

00:09:22.070 --> 00:09:24.890
This is something we can not so easily avoid.

00:09:24.889 --> 00:09:26.870
At least not by image processing alone.

00:09:26.870 --> 00:09:29.029
We could run some algorithms on

00:09:29.029 --> 00:09:32.000
the lidar point cloud itself and look for spatial proximity,

00:09:32.000 --> 00:09:35.120
but that's leading to far for this camera course and you

00:09:35.120 --> 00:09:38.345
should also have addressed it already in the lidar course.

00:09:38.345 --> 00:09:41.644
But what we have here is basically we have bounding box

00:09:41.644 --> 00:09:45.559
around this vehicle and those lidar points are within

00:09:45.559 --> 00:09:47.449
the same bounding box and they're not enclosed by

00:09:47.450 --> 00:09:49.550
any other bounding box because this vehicle here is

00:09:49.549 --> 00:09:54.169
not visible to the camera to the extent that we get an individual bounding box from it.

00:09:54.169 --> 00:09:56.854
So this is something we cannot avoid but

00:09:56.855 --> 00:10:00.995
actually we are only looking for computing the time to collision to the nearest object.

00:10:00.995 --> 00:10:03.440
This object at the far end actually,

00:10:03.440 --> 00:10:05.100
that's an object we don't care for currently.

00:10:05.100 --> 00:10:08.330
We want to know whether this area here comes closer and how

00:10:08.330 --> 00:10:13.780
quickly it does so and this vehicle is not keeping us from achieving this goal.

00:10:13.779 --> 00:10:18.220
So now we can group lidar points by using camera based regions of interest. That's cool.

00:10:18.220 --> 00:10:20.149
Now let's assemble everything to a set of

00:10:20.149 --> 00:10:24.329
3D objects from which we can extract some useful properties.

